{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ztchir/ztchir/blob/main/MECE_694_Office_Building_ENRG_Consump.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QQIwJd91XZa"
      },
      "source": [
        "Set up notebook to install kaggle datasets\n",
        "This only need to be run once at the start of the session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t05r9opoz9Kn"
      },
      "outputs": [],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LZBkdiK2NPc"
      },
      "outputs": [],
      "source": [
        "# If there is already a directory comment this cell\n",
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMZEG0Vk2cCS"
      },
      "outputs": [],
      "source": [
        "cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc5dYOUs28Ls"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEp0Vl4T3ca9"
      },
      "source": [
        "Download the Occupant presence and actions in office building Dataset\n",
        "\n",
        "For more info visit https://www.kaggle.com/claytonmiller/occupant-presence-and-actions-in-office-building\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LGRcjV01kDA"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download -d claytonmiller/occupant-presence-and-actions-in-office-building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmCmM4St4GQ1"
      },
      "outputs": [],
      "source": [
        "! unzip occupant-presence-and-actions-in-office-building.zip -d /content/drive/MyDrive/ColabNotebooks/MECE-694/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nf4hxxE44NDP"
      },
      "outputs": [],
      "source": [
        "! rm occupant-presence-and-actions-in-office-building.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7uFTzNx31uF"
      },
      "source": [
        "Import Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.19.5"
      ],
      "metadata": {
        "id": "XEshrBaTRMEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TkBmtAlARg8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLcDc0G13otl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px # for visualization\n",
        "\n",
        "import glob\n",
        "import re\n",
        "\n",
        "# Machine Learning Packages\n",
        "import sklearn as sk\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLxObz0z7Wml"
      },
      "source": [
        "Here we read the data from the separate csv files and combine them in to one data one not it that the data is taken at different time intervals for the some of the data. Outdoor temp and the like are takes on 15 minute intervals. Room, window and ligth data is taken at 15 minute intervals. To fill the missing data points pandas interpolation method is used with linearly interpolates the data on the 15, 30 and 45 minute intervals between the hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmNR64cW7SsO"
      },
      "outputs": [],
      "source": [
        "#path = r'drive.google.com/drive/my-drive/'\n",
        "path = r'/content/drive/MyDrive/ColabNotebooks/MECE-694/'\n",
        "all_files = glob.glob(path + '*.csv')\n",
        "\n",
        "li = []\n",
        "\n",
        "for filename in all_files:\n",
        "  df = pd.read_csv(filename, index_col=None, header=0)\n",
        "  df.index = pd.to_datetime(df['timestamp [dd/mm/yyyy HH:MM]'])\n",
        "  df.drop(['timestamp [dd/mm/yyyy HH:MM]'], axis=1, inplace=True)\n",
        "  df = df.resample('15T').interpolate()\n",
        "  li.append(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmXsJLc1OWWY"
      },
      "outputs": [],
      "source": [
        "li"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3Ia4iYGtgfw"
      },
      "outputs": [],
      "source": [
        "raw_dataset = pd.concat(li, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6h0djdGAULp"
      },
      "outputs": [],
      "source": [
        "raw_dataset.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMb1v6AzapSW"
      },
      "source": [
        "Here the columns are renamed for easy python use. The units and descriptions are saved in the units variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy2CeZUiQHFh"
      },
      "outputs": [],
      "source": [
        "raw_dataset.rename(columns={'wind speed [m/s]':'ws [m/s]'}, inplace=True)\n",
        "raw_dataset.rename(columns={'ki  [1:closed 0:open]':'ki [1:closed 0:open]'}, inplace=True)\n",
        "feature_info = {}\n",
        "new_cols = []\n",
        "plugs = []\n",
        "windows = []\n",
        "lights = []\n",
        "occ = []\n",
        "temp = []\n",
        "hum = []\n",
        "for col in raw_dataset.columns:\n",
        "  name, unit = col.split(' ', 1)                       \n",
        "  feature_info[name] = unit\n",
        "  if unit == '[C]' and name != 'tempOut':\n",
        "    new_cols.append(name + '_temp')\n",
        "    temp.append(name + '_temp')\n",
        "  elif unit == '[%]' and name != 'rh':\n",
        "    new_cols.append(name + '_hum')\n",
        "    hum.append(name + '_hum')\n",
        "  elif unit == '[W]':\n",
        "    new_cols.append(name + '_plug')\n",
        "    plugs.append(name + '_plug')\n",
        "  elif unit == '[1:closed 0:open]':\n",
        "    new_cols.append(name + '_window')\n",
        "    windows.append(name + '_window')\n",
        "  elif unit == '[0:off 1:on]':\n",
        "    new_cols.append(name + '_light')\n",
        "    lights.append(name + '_light')\n",
        "  elif unit == '[0:vacant 1:occupied]':\n",
        "    new_cols.append(name + '_occ')\n",
        "    occ.append(name + '_occ')\n",
        "  else:\n",
        "    new_cols.append(name)\n",
        "\n",
        "\n",
        "raw_dataset.columns = new_cols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjJthpRtcIUu"
      },
      "outputs": [],
      "source": [
        "continuous = ['tempOut', 'Wind', 'rh', 'gh', 'ws', 'plug_average', 'indoor_temp', 'indoor_humidity']\n",
        "window_states = ['o1_1_window', 'o1_2_window', 'o1_3_window', 'o1_4_window', 'o2_1_window', 'o2_2_window',\n",
        "                 'o3_1_window', 'o3_2_window', 'o3_3_window', 'o3_4_window', 'o4_1_window', 'o4_2_window',\n",
        "                 'mr_1_window', 'mr_2_window', 'mr_3_window', 'mr_4_window', 'mr_5_window', 'mr_6_window']\n",
        "state = ['occupancy', 'window_state', 'light_state']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iepGcB6U15k4"
      },
      "outputs": [],
      "source": [
        "raw_dataset[window_states].hist(figsize=(10,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHRTXT4y44pB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlt59FAZ46b2"
      },
      "source": [
        "Window state is predominantly closed. And when combined window states are closed at all times. Look at chaning window state per room."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3U7Oqg_hbn0"
      },
      "outputs": [],
      "source": [
        "raw_dataset.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rGiCwCUyt7p"
      },
      "outputs": [],
      "source": [
        "occ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdQL6Fxba10f"
      },
      "outputs": [],
      "source": [
        "raw_dataset['plug_average'] = raw_dataset[plugs].mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alQU04Try_3t"
      },
      "outputs": [],
      "source": [
        "raw_dataset['indoor_temp'] = raw_dataset[temp].mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx8gfTWh6iTP"
      },
      "outputs": [],
      "source": [
        "raw_dataset['occupancy'] = raw_dataset[occ].any(axis=1).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9mJthE1C3s4r"
      },
      "outputs": [],
      "source": [
        "raw_dataset['window_state'] = raw_dataset[windows].any(axis=1).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9rKw7HY9D8P"
      },
      "outputs": [],
      "source": [
        "raw_dataset['light_state'] = raw_dataset[lights].any(axis=1).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy0PJxXzz1Ps"
      },
      "outputs": [],
      "source": [
        "raw_dataset['indoor_humidity'] = raw_dataset[hum].mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaTpMS6S-n0I"
      },
      "outputs": [],
      "source": [
        "raw_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Df7WaMdT_NVt"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STnk6gqg9ZJ-"
      },
      "outputs": [],
      "source": [
        "clean_dataset = raw_dataset.drop(occ+windows+lights+temp+hum+plugs, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pja46vxEzx_H"
      },
      "outputs": [],
      "source": [
        "clean_dataset.to_csv(path + 'clean_data/clean_data.csv', index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQC-MGkTI9Yl"
      },
      "outputs": [],
      "source": [
        "clean_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLVejg8PddD6"
      },
      "source": [
        "Notes on dataset.\n",
        "\n",
        "\n",
        "\n",
        "*   Occupancy on 15 minute time interval Wind Direction on 1 hour. Use linear interpolation between hours to fill missing data. Another option would be to use occupancy in the past hour.\n",
        "*   Best to match the power load for the building format to maximize usable data.\n",
        "*  May be useful to use total Equipment power for the building a predicted value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T372C1PAa1i"
      },
      "outputs": [],
      "source": [
        "# plot the scatter matrix\n",
        "feature_names = raw_dataset.columns\n",
        "m = len(raw_dataset.columns) - 1\n",
        "fig = px.scatter_matrix(raw_dataset, dimensions=feature_names)\n",
        "\n",
        "fig.update_layout(width=m * 100,\n",
        "                  height = m * 100,\n",
        "                  margin=dict(l=0, r=0, t=0, b=0))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9mI1gh4AEVR"
      },
      "outputs": [],
      "source": [
        "raw_dataset.boxplot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stT37SYwvnbd"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "corr = clean_dataset.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqm6dHdbgqwG"
      },
      "source": [
        "Models\n",
        " * Data Analysis/Data Cleaning\n",
        " * Evolutionary Feature Selection- Maybe/Exploratory data analysis- Bowen \n",
        " * Linear Regression - Bowen\n",
        " ** Non-Linear - Bowen\n",
        " * Decision Tree - Bowen\n",
        " ** Random Forest - Feature Selection \n",
        " * SVM - Bowen/Zach\n",
        " * ANN - Zach\n",
        " ** CNN For Time Series Prediction - Zach\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Gq1uqt6lio"
      },
      "outputs": [],
      "source": [
        "clean_dataset.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HggyHztTkKe0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPpsWpBDAsx3"
      },
      "outputs": [],
      "source": [
        "clean_dataset[continuous].boxplot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bfpqglek0Qo"
      },
      "outputs": [],
      "source": [
        "clean_dataset[state].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyry8y1mknmq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKhTU7lx3qWm"
      },
      "source": [
        "Normalize the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxgc_Jw4jAbM"
      },
      "outputs": [],
      "source": [
        "normalized_data=(clean_dataset-clean_dataset.mean())/clean_dataset.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD79z4Xsjg-Z"
      },
      "outputs": [],
      "source": [
        "clean_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-I1rci_jtYg"
      },
      "source": [
        "The last few dataset cannot be interpolated and thus need to be removed due to nan values. The window state variable potentially indicates that at least one window is always open. We should revisit these variables. Maybe number of open windows?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fcl4A_2TjKSY"
      },
      "outputs": [],
      "source": [
        "normalized_data = normalized_data.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxNbiCN78LkF"
      },
      "outputs": [],
      "source": [
        "n = 3 # Drop last n columns due to NA values\n",
        "normalized_data.drop(normalized_data.tail(n).index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3FLbJU185MV"
      },
      "outputs": [],
      "source": [
        "normalized_data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVNxM29qBoPA"
      },
      "source": [
        "  For the following models we are working using time series modeling. Thus the data is not independent. Traditional ramdom data splitting will not work. In this case we will sequentially split the training and test data. We use 80% for tarining and validation and the remaining 20% for testing. \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIQ6BwSFMYcy"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW4eydBJM0FG"
      },
      "outputs": [],
      "source": [
        "# Convert dataset into predictors and labels\n",
        "X = normalized_data.drop(columns = ['window_state', 'plug_average']).to_numpy() # Features to test. Note here we can use past plug data for preditcion\n",
        "y = normalized_data['plug_average'].to_numpy() # Average Outlet Energy Consumption\n",
        "\n",
        "# # Perform test train split 80% 20%\n",
        "# X_train = X[:int(X.shape[0]*0.8)]\n",
        "# X_test = X[:int(X.shape[0]*0.2)]\n",
        "# y_train = y[:int(X.shape[0]*0.8)]\n",
        "# y_test = y[:int(X.shape[0]*0.2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDMRyMxpMrNw"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0:96].shape[1]"
      ],
      "metadata": {
        "id": "XO4DjulrKEwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bowen's Code to get consistent runtime. this is Bowen's code minus his data imports. This code uses the inputs of office occupancy data to predict the energy usage for a 15 minute interval."
      ],
      "metadata": {
        "id": "DQOllPShfvn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import Series,DataFrame\n",
        "from sklearn import preprocessing\n",
        "from pandas.core.frame import DataFrame\n",
        "import collections, numpy\n",
        "import math\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.svm import NuSVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy import stats\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from yellowbrick.model_selection import RFECV, rfecv\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from yellowbrick.model_selection import LearningCurve\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "\"\"\"3. Exploratory data analysis\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"4. Build ML models (linear regression/Random forest/SVM)\"\"\"\n",
        "\"\"\"4.1 Using linear regression for prediction\"\"\"\n",
        "reg = LinearRegression()\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "REG_pipe = Pipeline(steps=[('standardscaler', std), (\"classifier\", reg)])\n",
        "\n",
        "X_train = std.fit_transform(X_train)\n",
        "\n",
        "# # Grid search to find the proper paramters \n",
        "# parameters = {'classifier__C':list(range(0,1000,10)),'classifier__kernel':['linear', 'poly', 'rbf'], \n",
        "#               'classifier__gamma':np.array([0.001, 0.01, 0.1, 1])}\n",
        "\n",
        "# grid_search = GridSearchCV(REG_pipe, param_grid=parameters,scoring='r2',cv=10, n_jobs=-1)\n",
        "\n",
        "# grid_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# sorted(REG_pipe.get_params().keys())\n",
        "# # summarize results\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "# Validation using k-fold crossvalidation\n",
        "start = time.time()\n",
        "reg = LinearRegression()\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "REG_pipe = Pipeline(steps=[('standardscaler', std), (\"classifier\", reg)])\n",
        "\n",
        "X_train = std.fit_transform(X_train)\n",
        "\n",
        "validation_score = cross_validate(REG_pipe, X_train, y_train, scoring = \"r2\", cv= 10, return_train_score= True)\n",
        "\n",
        "validation_score['train_score']\n",
        "Training_score_mean = np.mean(validation_score['train_score'])\n",
        "print(Training_score_mean)\n",
        "\n",
        "validation_score['test_score']\n",
        "validation_score_mean = np.mean(validation_score['test_score'])\n",
        "print(validation_score_mean)\n",
        "print(validation_score)\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Training + validation time: {stop - start}s\")\n",
        "# =============================================================================\n",
        "# validation_score['train_score'], validation_score['test_score'] = validation_curve(\n",
        "#  svc, X_train, y_train, param_name=\"C\", param_range=(1,10,50,60,100,200,300,400,500,600,1000,2000,3000,\n",
        "#                                                      5000,10000), cv=10)\n",
        "# =============================================================================\n",
        "# Testing \n",
        "start = time.time()\n",
        "\n",
        "reg = LinearRegression()\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "REG_pipe = Pipeline(steps=[('standardscaler', std), (\"classifier\", reg)])\n",
        "\n",
        "REG_pipe.fit(X_train, y_train)\n",
        "\n",
        "X_test = std.fit_transform(X_test)\n",
        "\n",
        "y_predict = REG_pipe.predict(X_test)\n",
        "print('Linear Regression')\n",
        "print('r2:' , r2_score(y_test,y_predict))\n",
        "print(\"Mean absolute error:\", mean_absolute_error(y_test, y_predict))\n",
        "print(\"Root mean square error:\", mean_squared_error(y_test, y_predict, squared=False))\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Testing time: {stop - start}s\")\n",
        "\n",
        "\n",
        "\"\"\"4.2 Using SVM for prediction\"\"\"\n",
        "print('Support Vector Machine')\n",
        "svr = SVR()\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "SVR_pipe = Pipeline(steps=[('standardscaler', std), (\"classifier\", svr)])\n",
        "\n",
        "X_train = std.fit_transform(X_train)\n",
        "\n",
        "# # Grid search to find the proper paramters \n",
        "# parameters = {'classifier__C':list(range(0,1000,10)),'classifier__kernel':['linear', 'poly', 'rbf'], \n",
        "#               'classifier__gamma':np.array([0.001, 0.01, 0.1, 1])}\n",
        "\n",
        "# grid_search = GridSearchCV(SVR_pipe, param_grid= parameters,scoring='r2',cv=10, n_jobs=-1)\n",
        "\n",
        "# grid_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# sorted(REG_pipe.get_params().keys())\n",
        "# # summarize results\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "# Validation using k-fold crossvalidation\n",
        "start = time.time()\n",
        "svr = SVR(kernel='rbf')\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "SVR_pipe = Pipeline(steps=[('standardscaler', std), (\"classifier\", svr)])\n",
        "\n",
        "X_train = std.fit_transform(X_train)\n",
        "\n",
        "validation_score = cross_validate(SVR_pipe, X_train, y_train, scoring = 'r2', cv= 10, return_train_score= True)\n",
        "\n",
        "validation_score['train_score']\n",
        "Training_score_mean = np.mean(validation_score['train_score'])\n",
        "print(Training_score_mean)\n",
        "\n",
        "validation_score['test_score']\n",
        "validation_score_mean = np.mean(validation_score['test_score'])\n",
        "print(validation_score_mean)\n",
        "print(validation_score)\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Training + validation time: {stop - start}s\")\n",
        "# Testing \n",
        "start = time.time()\n",
        "\n",
        "svr = SVR(C=1.0, epsilon = 0.2)\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "SVR_pipe = Pipeline(steps=[('standardscaler', std), (\"classifier\", svr)])\n",
        "\n",
        "SVR_pipe.fit(X_train, y_train)\n",
        "\n",
        "X_test = std.fit_transform(X_test)\n",
        "\n",
        "y_predict = SVR_pipe.predict(X_test)\n",
        "\n",
        "print('r2:' , r2_score(y_test,y_predict))\n",
        "print(\"Mean absolute error:\", mean_absolute_error(y_test, y_predict))\n",
        "print(\"Root mean square error:\", mean_squared_error(y_test, y_predict, squared=False))\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Testing time: {stop - start}s\")\n",
        "\n",
        "\"\"\"4.3 Using RF for prediction\"\"\"\n",
        "print('Random Forest')\n",
        "rf = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "RF_pipe = Pipeline(steps=[('standardscaler', std), (\"classifier\", rf)])\n",
        "\n",
        "X_train = std.fit_transform(X_train)\n",
        "\n",
        "# # Grid search to find the proper paramters \n",
        "# parameters = {'classifier__C':list(range(0,1000,10)),'classifier__kernel':['linear', 'poly', 'rbf'], \n",
        "#               'classifier__gamma':np.array([0.001, 0.01, 0.1, 1])}\n",
        "\n",
        "# grid_search = GridSearchCV(RF_pipe, param_grid= parameters,scoring='r2',cv=10, n_jobs=-1)\n",
        "\n",
        "# grid_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# sorted(REG_pipe.get_params().keys())\n",
        "# # summarize results\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "# Validation using k-fold crossvalidation\n",
        "start = time.time()\n",
        "rf = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "RF_pipe = Pipeline(steps=[('standardscaler', std), (\"classifier\", rf)])\n",
        "\n",
        "X_train = std.fit_transform(X_train)\n",
        "\n",
        "validation_score = cross_validate(RF_pipe, X_train, y_train, scoring = \"r2\", cv= 10, return_train_score= True)\n",
        "\n",
        "validation_score['train_score']\n",
        "Training_score_mean = np.mean(validation_score['train_score'])\n",
        "print(Training_score_mean)\n",
        "\n",
        "validation_score['test_score']\n",
        "validation_score_mean = np.mean(validation_score['test_score'])\n",
        "print(validation_score_mean)\n",
        "print(validation_score)\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Training + validation time: {stop - start}s\")\n",
        "\n",
        "# Testing \n",
        "start = time.time()\n",
        "\n",
        "rf = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "RF_pipe = Pipeline(steps=[('standardscaler', std), (\"classifier\", rf)])\n",
        "\n",
        "RF_pipe.fit(X_train, y_train)\n",
        "\n",
        "X_test = std.fit_transform(X_test)\n",
        "\n",
        "y_predict = RF_pipe.predict(X_test)\n",
        "print('r2:' , r2_score(y_test,y_predict))\n",
        "print(\"Mean absolute error:\", mean_absolute_error(y_test, y_predict))\n",
        "print(\"Root mean square error:\", mean_squared_error(y_test, y_predict, squared=False))\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Testing time: {stop - start}s\")\n",
        "\n",
        "# MLPRegressor\n",
        "\n",
        "mlpreg = MLPRegressor(random_state=1, max_iter=500)\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "MLP_pipe = Pipeline(steps=[('stanadrdscalar', std), (\"classifier\", mlpreg)])\n",
        "\n",
        "X_train = std.fit_transform(X_train)\n",
        "\n",
        "# # Grid search to find the proper paramters \n",
        "# parameters = {'classifier__C':list(range(0,1000,10)),'classifier__kernel':['linear', 'poly', 'rbf'], \n",
        "#               'classifier__gamma':np.array([0.001, 0.01, 0.1, 1])}\n",
        "\n",
        "# grid_search = GridSearchCV(MLP_pipe, param_grid= parameters,scoring='r2',cv=10, n_jobs=-1)\n",
        "\n",
        "# grid_result = grid_search.fit(X_train, y_train)\n",
        "\n",
        "# sorted(REG_pipe.get_params().keys())\n",
        "# # summarize results\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "\n",
        "\n",
        "# Validation using k-fold crossvalidation\n",
        "start = time.time()\n",
        "mlpreg = MLPRegressor(random_state=1, max_iter=500)\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "MLP_pipe = Pipeline(steps=[('stanadrdscalar', std), (\"classifier\", mlpreg)])\n",
        "\n",
        "X_train = std.fit_transform(X_train)\n",
        "\n",
        "validation_score = cross_validate(MLP_pipe, X_train, y_train, scoring = \"r2\", cv= 10, return_train_score= True)\n",
        "\n",
        "validation_score['train_score']\n",
        "Training_score_mean = np.mean(validation_score['train_score'])\n",
        "print(Training_score_mean)\n",
        "\n",
        "validation_score['test_score']\n",
        "validation_score_mean = np.mean(validation_score['test_score'])\n",
        "print(validation_score_mean)\n",
        "print(validation_score)\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Training + validation time: {stop - start}s\")\n",
        "\n",
        "\n",
        "# Testing\n",
        "start = time.time()\n",
        "\n",
        "mlpreg = MLPRegressor(random_state=1, max_iter=500)\n",
        "\n",
        "MLP_pipe = Pipeline(steps=[('stanadrdscalar', std), (\"classifier\", mlpreg)])\n",
        "\n",
        "MLP_pipe.fit(X_train, y_train)\n",
        "\n",
        "X_test = std.fit_transform(X_test)\n",
        "\n",
        "y_predict = MLP_pipe.predict(X_test)\n",
        "print('MLP Regressor')\n",
        "print('r2:' , r2_score(y_test,y_predict))\n",
        "print(\"Mean absolute error:\", mean_absolute_error(y_test, y_predict))\n",
        "print(\"Root mean square error:\", mean_squared_error(y_test, y_predict, squared=False))\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Testing time: {stop - start}s\")"
      ],
      "metadata": {
        "id": "dPklCPcNKloj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code we do not consider that the data we have is time dependent. With time series data we are able to use past data in order to help predict the future energy consumption in the data. The test train split and models are differnt. We must create a suitable train and test split for a Time series data set. Here we save the last months worth of data and use it for testing. The remaining data is used for trainng our models. To create our data we use a the past 24 hours of data to predict the next hour of consuption on 15 minute time intervals. We will look at various models using the time series data. MLP Regressor, CNNs and Deep NN with LTSM layers layers."
      ],
      "metadata": {
        "id": "hNR6NKNog7Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TimeSeries Split \n",
        "# tscv = TimeSeriesSplit(\n",
        "#     n_splits=5,\n",
        "#     max_train_size=10000,\n",
        "#     test_size=1000,\n",
        "#     ) # Create test train split that will train on previous 24 hours of opservations and predict the next hour\n",
        "# print(tscv)\n",
        "\n",
        "# for train_index, val_index in tscv.split(X_train):\n",
        "#   X_tr, X_val = X_train[train_index], X_train[val_index]\n",
        "#   y_tr, y_val = y_train[train_index], y_train[train_index]\n",
        "\n",
        "# print(X_tr.shape)\n"
      ],
      "metadata": {
        "id": "vOPYy4yqg5Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dataset into predictors and labels\n",
        "X = normalized_data.drop(columns = ['window_state']).to_numpy() # Features to test. Note here we can use past plug data for preditcion\n",
        "y = normalized_data['plug_average'].to_numpy() # Average Outlet Energy Consumption"
      ],
      "metadata": {
        "id": "S7q04KNIb8tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(X, y):\n",
        "    input_size = 96 # The past 24 hours of data each of the 96 entries will have n features\n",
        "    test_size = 2880 # 30 Last 30 Days of the data set. 30 days * 24 Hours * 4 Quarter Hours\n",
        "    output_size = 4\n",
        "    try:\n",
        "      num_features = X.shape[1]\n",
        "    except:\n",
        "      num_features = 1\n",
        "\n",
        "    X_samples = np.empty([len(X)-input_size-test_size-(input_size + output_size), input_size*num_features])\n",
        "    y_samples = np.empty((len(X)-input_size-test_size-(input_size + output_size), output_size))\n",
        "    for step_ahead in range(0,len(X)-input_size-test_size-(input_size + output_size)):\n",
        "        X_samples[step_ahead,:] = X[step_ahead:step_ahead+input_size].flatten()\n",
        "        y_samples[step_ahead,:] = y[step_ahead+input_size:step_ahead+input_size+output_size]\n",
        "\n",
        "    \n",
        "    train_size = int(len(X) - input_size - test_size)\n",
        "\n",
        "    X_test = np.empty((test_size,input_size*num_features))\n",
        "    y_test = np.empty((test_size, output_size))\n",
        "    for step_ahead in range(0,test_size-3):\n",
        "      X_test[step_ahead,:] =  X[step_ahead+len(X)-test_size-input_size:step_ahead+len(X)-test_size].flatten()\n",
        "      y_test[step_ahead,:] = y[step_ahead+len(X)-test_size:step_ahead+len(X)-test_size+output_size]\n",
        "\n",
        "    X_train = X_samples[0:train_size,:]\n",
        "    y_train = y_samples[0:train_size,:]\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test"
      ],
      "metadata": {
        "id": "GEMKBaAOIDcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = generate_samples(X,y)"
      ],
      "metadata": {
        "id": "eOPuyCyfLYti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "C5lIkR0Tefa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TyFBt01rZFj"
      },
      "outputs": [],
      "source": [
        "n_points = 100\n",
        "X = np.random.randn(100, 10)\n",
        "\n",
        "percentiles_classes = [0.1, 0.3, 0.6]\n",
        "y = np.hstack([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])\n",
        "\n",
        "groups = np.hstack([[ii] * 1000 for ii in range(len(X_train))])\n",
        "groups.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rb7xTC2kUi1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "cmap_data = plt.cm.Paired\n",
        "cmap_cv = plt.cm.coolwarm\n",
        "n_splits = 4\n",
        "\n",
        "# Evenly spaced groups repeated once\n",
        "groups = np.hstack([[ii] * 10 for ii in range(10)])\n",
        "\n",
        "\n",
        "def visualize_groups(classes, groups, name):\n",
        "    # Visualize dataset groups\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.scatter(\n",
        "        range(len(groups)),\n",
        "        [0.5] * len(groups),\n",
        "        c=groups,\n",
        "        marker=\"_\",\n",
        "        lw=50,\n",
        "        cmap=cmap_data,\n",
        "    )\n",
        "    ax.scatter(\n",
        "        range(len(groups)),\n",
        "        [3.5] * len(groups),\n",
        "        c=classes,\n",
        "        marker=\"_\",\n",
        "        lw=50,\n",
        "        cmap=cmap_data,\n",
        "    )\n",
        "    ax.set(\n",
        "        ylim=[-1, 5],\n",
        "        yticks=[0.5, 3.5],\n",
        "        yticklabels=[\"Data\\ngroup\", \"Data\\nclass\"],\n",
        "        xlabel=\"Sample index\",\n",
        "    )\n",
        "\n",
        "\n",
        "visualize_groups(y, groups, \"no groups\")\n",
        "\n",
        "def plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n",
        "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
        "\n",
        "    # Generate the training/testing visualizations for each CV split\n",
        "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n",
        "        # Fill in indices with the training/test groups\n",
        "        indices = np.array([np.nan] * len(X))\n",
        "        indices[tt] = 1\n",
        "        indices[tr] = 0\n",
        "\n",
        "        # Visualize the results\n",
        "        ax.scatter(\n",
        "            range(len(indices)),\n",
        "            [ii + 0.5] * len(indices),\n",
        "            c=indices,\n",
        "            marker=\"_\",\n",
        "            lw=lw,\n",
        "            cmap=cmap_cv,\n",
        "            vmin=-0.2,\n",
        "            vmax=1.2,\n",
        "        )\n",
        "\n",
        "    # Plot the data classes and groups at the end\n",
        "    ax.scatter(\n",
        "        range(len(X)), [ii + 1.5] * len(X), c=y, marker=\"_\", lw=lw, cmap=cmap_data\n",
        "    )\n",
        "\n",
        "    ax.scatter(\n",
        "        range(len(X)), [ii + 2.5] * len(X), c=group, marker=\"_\", lw=lw, cmap=cmap_data\n",
        "    )\n",
        "\n",
        "    # Formatting\n",
        "    yticklabels = list(range(n_splits)) + [\"class\", \"group\"]\n",
        "    ax.set(\n",
        "        yticks=np.arange(n_splits + 2) + 0.5,\n",
        "        yticklabels=yticklabels,\n",
        "        xlabel=\"Sample index\",\n",
        "        ylabel=\"CV iteration\",\n",
        "        ylim=[n_splits + 2.2, -0.2],\n",
        "        xlim=[0, 100],\n",
        "    )\n",
        "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=15)\n",
        "    return ax\n",
        "\n",
        "cvs = [\n",
        "    TimeSeriesSplit,\n",
        "]\n",
        "\n",
        "for cv in cvs:\n",
        "    this_cv = cv(n_splits=n_splits)\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n",
        "\n",
        "    ax.legend(\n",
        "        [Patch(color=cmap_cv(0.8)), Patch(color=cmap_cv(0.02))],\n",
        "        [\"Testing set\", \"Training set\"],\n",
        "        loc=(1.02, 0.8),\n",
        "    )\n",
        "    # Make the legend fit\n",
        "    plt.tight_layout()\n",
        "    fig.subplots_adjust(right=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQevqTq8vckI"
      },
      "outputs": [],
      "source": [
        "int(X_train.shape[0]/(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nK1r8sQJTSi2"
      },
      "outputs": [],
      "source": [
        "# # Use TimeSeries Split \n",
        "# tscv = TimeSeriesSplit(\n",
        "#     n_splits=int(X_train.shape[0]/4),\n",
        "#     gap=0,\n",
        "#     max_train_size=4*24, # Consumption to be predicted based on last 24 hour or 96 time steps\n",
        "#     test_size=4,  # Predict the outlet consumption for the next hour\n",
        "#     ) # Create test train split that will train on previous 24 hours of opservations and predict the next hour\n",
        "# print(tscv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVkDc3vLQNGt"
      },
      "outputs": [],
      "source": [
        "# for train_index, val_index in tscv.split(X_train):\n",
        "#   X_tr, X_val = X_train[train_index], X_train[val_index]\n",
        "#   y_tr, y_val = y_train[train_index], y_train[val_index]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qErGT1V1VMc"
      },
      "source": [
        "To compare the models we will be using $R^2$ Score. $R^2$ score $R^2 = 1 - \\frac{RSS}{TSS}$ = $1 - \\frac{\\Sigma(y_i - \\hat{y_i})^2}{\\Sigma(y_i-\\bar{y_i})^2}$. In this case a perfect model will have an RSS of 1 and a baseline RSS of 0, a negative value means the model performs worse than the baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohqk14cCytiJ"
      },
      "source": [
        "Here we train a random forest regressor usnig time series data to predict the next 4 hours of plug consumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWYsCwLz3sjM"
      },
      "outputs": [],
      "source": [
        "# # Use TimeSeries Split \n",
        "# tscv = TimeSeriesSplit(\n",
        "#     n_splits=int(X_train.shape[0]/4),\n",
        "#     gap=0,\n",
        "#     max_train_size=4*24, # Consumption to be predicted based on last 24 hour or 96 time steps\n",
        "#     test_size=4,  # Predict the outlet consumption for the next hour\n",
        "#     ) # Create test train split that will train on previous 24 hours of opservations and predict the next hour\n",
        "# print(tscv)\n",
        "\n",
        "# i = 1\n",
        "# score = []\n",
        "\n",
        "# for mf in np.linspace(8, 10, 1):\n",
        "#   for ne in np.linspace(50, 100, 6):\n",
        "#     for md in np.linspace(20, 40, 5):\n",
        "#       for msl in np.linspace(30, 100, 8):\n",
        "#         rfr = RandomForestRegressor(\n",
        "#             max_features = int(mf),\n",
        "#             n_estimators=int(ne),\n",
        "#             max_depth=int(md),\n",
        "#             min_samples_leaf = int(msl))\n",
        "#         rfr.fit(X_train, y_train)\n",
        "#         score.append([i,\n",
        "#                       mf,\n",
        "#                       ne,\n",
        "#                       md,\n",
        "#                       msl,\n",
        "#                       rfr.score(X_test,y_test)])\n",
        "#   print(score[i])        \n",
        "#   i += 1\n",
        "rfr = RandomForestRegressor(random_state=1, max_depth=2).fit(X_train, y_train)\n",
        "regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Cross Validation\n",
        "# Validation using k-fold crossvalidation\n",
        "# Validation using k-fold crossvalidation\n",
        "start = time.time()\n",
        "rf = RandomForestRegressor(max_depth=2, random_state=0)\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "RF_pipe = Pipeline(steps=[('standardscaler', std), (\"classifier\", rf)])\n",
        "\n",
        "X_train = std.fit_transform(X_train)\n",
        "\n",
        "validation_score = cross_validate(RF_pipe, X_train, y_train, scoring = \"r2\", cv= 10, return_train_score= True)\n",
        "\n",
        "validation_score['train_score']\n",
        "Training_score_mean = np.mean(validation_score['train_score'])\n",
        "print(Training_score_mean)\n",
        "\n",
        "validation_score['test_score']\n",
        "validation_score_mean = np.mean(validation_score['test_score'])\n",
        "print(validation_score_mean)\n",
        "print(validation_score)\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Training + validation time: {stop - start}s\")\n",
        "\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "rfr = RandomForestRegressor(random_state=1, max_depth=2).fit(X_train, y_train)\n",
        "#rfr.score(X_test, y_test)\n",
        "y_pred = rfr.predict(X_test)\n",
        "\n",
        "print('MLP Regressor Time Series')\n",
        "print('r2:' , r2_score(y_test, y_pred))\n",
        "print(\"Mean absolute error:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"Root mean square error:\", mean_squared_error(y_test, y_pred, squared=False))\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Testing time: {stop - start}s\")\n"
      ],
      "metadata": {
        "id": "G1MkMSzrcngM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "qjwt3vSwcE5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation using k-fold crossvalidation\n",
        "start = time.time()\n",
        "mlpreg = MLPRegressor(random_state=1, max_iter=500)\n",
        "\n",
        "std = StandardScaler()\n",
        "\n",
        "MLP_pipe = Pipeline(steps=[('stanadrdscalar', std), (\"classifier\", mlpreg)])\n",
        "\n",
        "X_train = std.fit_transform(X_train)\n",
        "\n",
        "validation_score = cross_validate(MLP_pipe, X_train, y_train, scoring = \"r2\", cv= 10, return_train_score= True)\n",
        "\n",
        "validation_score['train_score']\n",
        "Training_score_mean = np.mean(validation_score['train_score'])\n",
        "print(Training_score_mean)\n",
        "\n",
        "validation_score['test_score']\n",
        "validation_score_mean = np.mean(validation_score['test_score'])\n",
        "print(validation_score_mean)\n",
        "print(validation_score)\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Training + validation time: {stop - start}s\")\n",
        "\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)\n",
        "#regr.score(X_test, y_test)\n",
        "y_pred = regr.predict(X_test)\n",
        "\n",
        "print('MLP Regressor Time Series')\n",
        "print('r2:' , r2_score(y_test,y_pred))\n",
        "print(\"Mean absolute error:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"Root mean square error:\", mean_squared_error(y_test, y_pred, squared=False))\n",
        "\n",
        "stop = time.time()\n",
        "print(f\"Testing time: {stop - start}s\")"
      ],
      "metadata": {
        "id": "NA7ElZxdp9Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V1XBqdQzUv2"
      },
      "source": [
        "Next we use an MLPRegressor for predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yVF2H-dQim2"
      },
      "outputs": [],
      "source": [
        "# i = 1\n",
        "# score = []\n",
        "\n",
        "# for hl in np.linspace(1, 100, 1):\n",
        "#   for slvr in ['lbfgs', 'sdg', 'adam']:\n",
        "#     for al in np.linspace(0.0001, 1, 20):\n",
        "#       for lr in np.linspace(0.0001, 0.01, 10):\n",
        "#         regr = MLPRegressor(\n",
        "#             random_state=1,\n",
        "#             max_iter=10000,\n",
        "#             hidden_layer_sizes=int(hl),\n",
        "#             solver=slvr,\n",
        "#             alpha=al,\n",
        "#             learning_rate_init=lr,\n",
        "#             )\n",
        "#         regr.fit(X_train, y_train)\n",
        "#         score.append([i,\n",
        "#                       mf,\n",
        "#                       ne,\n",
        "#                       md,\n",
        "#                       msl,\n",
        "#                       regr.score(X_test,y_test)])\n",
        "#     print(score[i])        \n",
        "#     i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FU8umn62_CsI"
      },
      "outputs": [],
      "source": [
        "y_pred_mlp = regr.predict(X_test)\n",
        "y_pred_rfr = rfr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_yjHELF_HZ5"
      },
      "outputs": [],
      "source": [
        "plt.plot(y_test, label='True Values')\n",
        "plt.plot(y_pred_mlp, label='MLP Predicted Values')\n",
        "plt.plot(y_pred_rfr, label='Random Forest Prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "626jYydLxNYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CGGntyWvC31h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten"
      ],
      "metadata": {
        "id": "Q4wK-_NQsWH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7-8hJI5TvrJ"
      },
      "source": [
        "Next look at Deep RNN using LSTM layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = normalized_data['plug_average'].to_numpy() # Average Outlet Energy Consumption # Features to test. Note here we can use past plug data for preditcion\n",
        "y = normalized_data['plug_average'].to_numpy() # Average Outlet Energy Consumption"
      ],
      "metadata": {
        "id": "YfLMP7CiuaBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = generate_samples(X,y)"
      ],
      "metadata": {
        "id": "tv3DCKVruUg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = path + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "I4Ro6t-3vSDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0].shape"
      ],
      "metadata": {
        "id": "v3bvOc96vfYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv9-liUgTtfv"
      },
      "outputs": [],
      "source": [
        "deeprnn = keras.models.Sequential([\n",
        "    keras.layers.SimpleRNN(96, return_sequences=True, input_shape=[None, 1]),\n",
        "    keras.layers.SimpleRNN(500, return_sequences=True),\n",
        "    keras.layers.Dense(4)                                   \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deeprnn.compile(loss=\"mean_squared_error\",\n",
        "                optimizer=\"sgd\",\n",
        "                metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
        "                )"
      ],
      "metadata": {
        "id": "Q7lYX1gRW3Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_deeprnn = deeprnn.fit(X_train,y_train, epochs = 20,\n",
        "                              validation_split=0.3, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "yhJPz8tYXOTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = Sequential([\n",
        "    keras.layers.LSTM(200, return_sequences=True, input_shape=[None,1]),\n",
        "    keras.layers.LSTM(200),\n",
        "    keras.layers.Dense(100)\n",
        "])"
      ],
      "metadata": {
        "id": "TyeBW7yknbkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_lstm = ltsm.fit(X_train, y_train, epochs = 20,\n",
        "                        validation_split=0.3, callbacks = [tensorboard_callback])"
      ],
      "metadata": {
        "id": "CmRuaMM3ngUE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MECE-694-Office-Building-ENRG-Consump.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}